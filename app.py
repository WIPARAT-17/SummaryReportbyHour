import requests
import re
import html
import json
import xml.etree.ElementTree as ET
from ftfy import fix_text
import io
import csv
import datetime
import os
import argparse
import pandas as pd
from flask import Flask, request, render_template, jsonify, send_from_directory, send_file, send_from_directory
import tempfile
import threading
import uuid
from reportlab.lib.pagesizes import letter, landscape # ‡πÄ‡∏û‡∏¥‡πà‡∏° landscape ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
from reportlab.platypus import SimpleDocTemplate, Table, Paragraph, Spacer, PageBreak ,TableStyle
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
import logging
from queue import Queue
import zipfile
import shutil
import time

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Flask application
app = Flask(__name__)

# --- ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞ Lock ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Thread-safe ---
# `processing_status` ‡πÄ‡∏Å‡πá‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ job_id ‡πÄ‡∏õ‡πá‡∏ô key
# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:
# {
#     'job_id_1': {'total': 100, 'processed': 10, 'completed': False, 'error': None, 'canceled': False, 'results': [], 'temp_dir': '/tmp/report_job_xyz', 'zip_file_path': None, 'timestamp': datetime_obj},
#     'job_id_2': {...}
# }
processing_status = {}
# `status_lock` ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á `processing_status` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Race Condition ‡πÉ‡∏ô Multi-threading
status_lock = threading.Lock()

# --- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Logger ‡πÅ‡∏•‡∏∞ Log Queue ---
# `log_queue` ‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° log ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÅ‡∏ö‡∏ö Real-time
log_queue = Queue()

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logger ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡πÄ‡∏ä‡πà‡∏ô INFO, WARNING, ERROR)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO) # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö log ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á

UPLOAD_FOLDER = 'uploads'
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER

# ‡∏•‡πâ‡∏≤‡∏á handler ‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô log ‡∏ã‡πâ‡∏≥
if logger.handlers:
    for handler in list(logger.handlers):
        logger.removeHandler(handler)

# Custom Log Handler ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° log ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Queue
class QueueHandler(logging.Handler):
    def __init__(self, queue):
        super().__init__()
        self.queue = queue

    def emit(self, record):
        try:
            msg = self.format(record) # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° log

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö
            if "üìÅ ‡∏•‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå CSV/PDF ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß" in msg:
                return # ‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡∏•‡∏á‡πÉ‡∏ô‡∏Ñ‡∏¥‡∏ß

            # ‡πÉ‡∏ä‡πâ Regular Expression ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á log (‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤ timestamp, level, job_id)
            log_pattern = re.compile(r"^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3} - (INFO|WARNING|ERROR|CRITICAL) - (Job [0-9a-f-]+: )?(.*)")
            match = log_pattern.match(msg)
            if match:
                clean_msg = match.group(3) # ‡∏î‡∏∂‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å
                self.queue.put(clean_msg) # ‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏á‡πÉ‡∏ô‡∏Ñ‡∏¥‡∏ß
            else:
                self.queue.put(msg) # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á pattern ‡∏Å‡πá‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ï‡πá‡∏°‡∏•‡∏á‡πÑ‡∏õ

        except Exception:
            self.handleError(record) # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô handler ‡∏ô‡∏µ‡πâ

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Console Handler ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ log ‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô Console/Terminal ‡∏î‡πâ‡∏ß‡∏¢
console_handler = logging.StreamHandler()
console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
logger.addHandler(console_handler)

# ‡πÄ‡∏û‡∏¥‡πà‡∏° QueueHandler ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô logger ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ log ‡πÑ‡∏õ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Queue ‡∏î‡πâ‡∏ß‡∏¢
queue_handler = QueueHandler(log_queue)
logger.addHandler(queue_handler)

# --- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PDF ---
THAI_FONT_NAME = 'THSarabunNew' # ‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÉ‡∏ô ReportLab
THAI_FONT_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'THSarabunNew.ttf') # Path ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ü‡∏≠‡∏ô‡∏ï‡πå

THAI_FONT_REGISTERED = False
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
if os.path.exists(THAI_FONT_PATH):
    try:
        # ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡∏Å‡∏±‡∏ö ReportLab ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô PDF ‡πÑ‡∏î‡πâ
        pdfmetrics.registerFont(TTFont(THAI_FONT_NAME, THAI_FONT_PATH))
        THAI_FONT_REGISTERED = True
        #logger.info(f"Thai font '{THAI_FONT_NAME}' registered successfully from '{THAI_FONT_PATH}'.")
    except Exception as e:
        logger.error(f"ERROR: Could not register Thai font '{THAI_FONT_NAME}'. Error: {e}")
else:
    logger.warning(f"WARNING: Thai font file '{THAI_FONT_PATH}' not found. Please ensure the font file is in the same directory as the script.")

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---
def get_data_from_api(nod_id, itf_id, job_id):
    """
    ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ß‡∏á‡∏à‡∏£‡∏à‡∏≤‡∏Å API ‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å (SOAP-based) ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô JSON

    Parameters:
    - nod_id (str): Node ID ‡∏Ç‡∏≠‡∏á‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå
    - itf_id (str): Interface ID ‡∏Ç‡∏≠‡∏á Interface
    - job_id (str): ID ‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö logging

    Returns:
    - dict: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å API ‡∏´‡∏£‡∏∑‡∏≠ None ‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î
    """
     # 1. ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô URL ‡∏Ç‡∏≠‡∏á API ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô v2
    url = "http://1.179.233.116:8082/api_csoc_02/server_solarwinds_ginv2.php"

    headers = {
        # 2. ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô SOAPAction ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö URL ‡πÉ‡∏´‡∏°‡πà
        "SOAPAction": "http://1.179.233.116/api_csoc_02/server_solarwinds_ginv2.php/circuitStatus",
        "Content-Type": "text/xml; charset=utf-8",
    }
    # 3. ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Namespace (xmlns) ‡πÉ‡∏ô SOAP Body ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô v2
    body = f"""<?xml version="1.0" encoding="utf-8"?>
<soap:Envelope xmlns:soap="http://schemas.xmlsoap.org/soap/envelope/"
               xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
               xmlns:xsd="http://www.w3.org/2001/XMLSchema">
  <soap:Body>
    <circuitStatus xmlns="http://1.179.233.116/soap/#Service_Solarwinds_ginv2">
      <nodID>{nod_id}</nodID>
      <itfID>{itf_id}</itfID>
    </circuitStatus>
  </soap:Body>
</soap:Envelope>"""

    # -------------------- END: ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç --------------------

    try:
        # ‡∏™‡πà‡∏á POST Request ‡πÑ‡∏õ‡∏¢‡∏±‡∏á API ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• SOAP Body ‡πÅ‡∏•‡∏∞ Headers
        resp = requests.post(url, data=body, headers=headers, timeout=10)
        resp.raise_for_status() # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Request ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (HTTP 2xx)

        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ XML Response ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö
        match = re.search(r"(<\?xml.*?</SOAP-ENV:Envelope>)", resp.text, re.DOTALL)
        if not match:
            logger.warning(f"‡πÑ‡∏°‡πà‡∏û‡∏ö XML Response ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NodeID: {nod_id}, Interface ID: {itf_id}")
            return None

        # Parse XML Response ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô 'return'
        root = ET.fromstring(match.group(1))
        return_tag = root.find(".//{*}return")
        if return_tag is None or not return_tag.text:
            logger.warning(f"API ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NodeID: {nod_id}, Interface ID: {itf_id}")
            return None

        raw_text = return_tag.text
        # Unescape HTML entities (e.g., &quot; becomes ")
        html_unescaped = html.unescape(raw_text)
        # Fix encoding issues that might arise from unicode escape sequences
        fixed_text = fix_text(bytes(html_unescaped, "utf-8").decode("unicode_escape"))
        parsed_json = json.loads(fixed_text) # ‡πÅ‡∏õ‡∏•‡∏á String JSON ‡πÄ‡∏õ‡πá‡∏ô Python Dictionary/List
        return parsed_json
    except requests.exceptions.RequestException as req_e:
        logger.error(f"‚ùå ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NodeID: {nod_id}, Interface ID: {itf_id} ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {req_e}")
        return None
    except ET.ParseError as parse_e:
        logger.error(f"‚ùå XML Parsing ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NodeID: {nod_id}, Interface ID: {itf_id}: {parse_e}")
        return None
    except json.JSONDecodeError as json_e:
        logger.error(f"‚ùå JSON Decoding ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NodeID: {nod_id}, Interface ID: {itf_id}: {json_e}")
        return None
    except Exception as e:
        logger.error(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NodeID: {nod_id}, Interface ID: {itf_id}: {e}")
        return None

def process_json_data(raw_json_data, job_id, excel_node_id, excel_agency_name):
    """
    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å API ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå CSV/PDF
    - ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö 24 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏£‡∏ß‡∏° (Grand Total Average) ‡∏Ç‡∏≠‡∏á In_Averagebps ‡πÅ‡∏•‡∏∞ Out_Averagebps
    - ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Bandwidth ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

    Parameters:
    - raw_json_data (list or dict): ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON ‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å API
    - job_id (str): ID ‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö logging
    - excel_node_id (str): Node ID ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel (‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏´‡∏≤‡∏Å API ‡πÑ‡∏°‡πà‡∏°‡∏µ Customer_Curcuit_ID)
    - excel_agency_name (str): ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå Excel (‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏´‡∏≤‡∏Å API ‡πÑ‡∏°‡πà‡∏°‡∏µ Address)

    Returns:
    - tuple: (headers, processed_data, grand_total_row)
        - headers (list): ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô CSV/PDF
        - processed_data (list): ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡∏£‡∏ß‡∏° 24 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á)
        - grand_total_row (dict): ‡πÅ‡∏ñ‡∏ß Grand Total (Average)
    """
    # ------------------- START: ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç -------------------

    # 1. ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï key ‡∏Ç‡∏≠‡∏á "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤" ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö API ‡πÉ‡∏´‡∏°‡πà ‡∏Ñ‡∏∑‡∏≠ 'Timestamp'
    column_mapping = {
        "‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô": "Customer_Curcuit_ID",
        "‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô": "Address",
        "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤": "Timestamp", # <--- ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å 'Timestamp'
        "‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)": "Bandwidth",
        "In_Averagebps": "In_Averagebps",
        "Out_Averagebps": "Out_Averagebps"
    }
    # -------------------- END: ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç --------------------

    desired_headers_th = list(column_mapping.keys())

    if raw_json_data is None or (isinstance(raw_json_data, list) and not raw_json_data):
        logger.warning(f"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Job {job_id}")
        return desired_headers_th, [], {}

    data_to_process = raw_json_data if isinstance(raw_json_data, list) else [raw_json_data]

    api_customer_circuit_id = excel_node_id
    address_to_use = excel_agency_name
    bandwidth_to_use = ''

    first_item = data_to_process[0]

    api_customer_circuit_id = first_item.get(column_mapping["‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô"]) or excel_node_id
    api_address = first_item.get(column_mapping["‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô"]) or excel_agency_name
    bandwidth_raw_from_api = first_item.get(column_mapping["‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)"]) or ''

    if "FTTx" in str(bandwidth_raw_from_api):
        bandwidth_to_use = "20 Mbps."
    else:
        try:
            numeric_value_match = re.search(r'[\d.]+', str(bandwidth_raw_from_api))
            if numeric_value_match:
                numeric_value = float(numeric_value_match.group())
                bandwidth_to_use = f"{int(numeric_value)} Mbps."
            else:
                bandwidth_to_use = str(bandwidth_raw_from_api)
        except (ValueError, TypeError, AttributeError):
            bandwidth_to_use = str(bandwidth_raw_from_api)

    min_date_from_api = None
    max_date_from_api = None

    # ------------------- START: ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç -------------------
    # 2. ‡∏õ‡∏£‡∏±‡∏ö logic ‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å 'Timestamp' ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô string ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
    for item in data_to_process:
        timestamp_str = item.get('Timestamp')
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Timestamp ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á
        if timestamp_str:
            try:
                # ‡∏•‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏ß‡∏•‡∏≤‡πÇ‡∏î‡∏¢‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏õ‡πá‡∏ô 'YYYY-MM-DD HH:MM:SS'
                # ‡∏´‡∏≤‡∏Å API ‡∏™‡πà‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏≠‡∏∑‡πà‡∏ô‡∏°‡∏≤ ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö format string ('%d/%m/%Y %H') ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
                dt_obj_from_api = datetime.datetime.strptime(timestamp_str, '%d/%m/%Y %H')
                if min_date_from_api is None or dt_obj_from_api.date() < min_date_from_api:
                    min_date_from_api = dt_obj_from_api.date()
                if max_date_from_api is None or dt_obj_from_api.date() > max_date_from_api:
                    max_date_from_api = dt_obj_from_api.date()
            except (ValueError, TypeError):
                logger.warning(f"Job {job_id}: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≤‡∏Å Timestamp ‡πÑ‡∏î‡πâ: '{timestamp_str}'")
                continue
    # -------------------- END: ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç --------------------

    if min_date_from_api is None or max_date_from_api is None:
        logger.warning(f"Job {job_id}: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏≤‡∏Å API ‡πÄ‡∏•‡∏¢ ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô")
        today = datetime.date.today()
        first_day_of_current_month = today.replace(day=1)
        report_end_date = first_day_of_current_month - datetime.timedelta(days=1)
        report_start_date = report_end_date.replace(day=1)
    else:
        report_start_date = min_date_from_api.replace(day=1)
        next_month_start_for_max = (max_date_from_api.replace(day=1) + datetime.timedelta(days=32)).replace(day=1)
        report_end_date = next_month_start_for_max - datetime.timedelta(days=1)

    full_data_structure = {}
    current_date = report_start_date
    while current_date <= report_end_date:
        for hour in range(24):
            dt_obj = datetime.datetime.combine(current_date, datetime.time(hour, 0, 0))
            formatted_date_time = dt_obj.strftime('%Y-%m-%d %H.%M.%S')
            date_key = current_date.strftime('%Y-%m-%d')

            if date_key not in full_data_structure:
                full_data_structure[date_key] = {}

            full_data_structure[date_key][formatted_date_time] = {
                "‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô": api_customer_circuit_id,
                "‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô": api_address,
                "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤": formatted_date_time,
                "‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)": bandwidth_to_use,
                "In_Averagebps": "0",
                "Out_Averagebps": "0",
                "_raw_incoming": 0,
                "_raw_outcoming": 0
            }
        current_date += datetime.timedelta(days=1)

    # ------------------- START: ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç -------------------
    # 3. ‡∏õ‡∏£‡∏±‡∏ö logic ‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏≤‡∏Å 'Timestamp' ‡πÄ‡∏ä‡πà‡∏ô‡∏Å‡∏±‡∏ô
    for item in data_to_process:
        dt_obj_from_api = None
        timestamp_str = item.get('Timestamp')
        if timestamp_str:
            try:
                dt_obj_from_api = datetime.datetime.strptime(timestamp_str, '%d/%m/%Y %H')
            except (ValueError, TypeError):
                continue

        if dt_obj_from_api:
            # ‡πÉ‡∏ä‡πâ dt_obj_from_api ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏°‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πà‡∏≠‡πÑ‡∏õ...
            # (‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÉ‡∏ô‡∏•‡∏π‡∏õ‡∏ô‡∏µ‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç)
    # -------------------- END: ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç --------------------
            date_key = dt_obj_from_api.strftime('%Y-%m-%d')
            # ‡∏õ‡∏£‡∏±‡∏ö format ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô H.M.S ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö key ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ
            formatted_time_from_api = dt_obj_from_api.strftime('%Y-%m-%d %H.%M.%S')


            if date_key in full_data_structure and formatted_time_from_api in full_data_structure[date_key]:
                item_customer_id = item.get(column_mapping["‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô"])
                item_address = item.get(column_mapping["‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô"])

                if item_customer_id:
                    full_data_structure[date_key][formatted_time_from_api]["‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô"] = item_customer_id

                if item_address:
                    full_data_structure[date_key][formatted_time_from_api]["‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô"] = item_address

                in_avg_bps = item.get('In_Averagebps', '0')
                out_avg_bps = item.get('Out_Averagebps', '0')

                try:
                    in_avg_bps_float = float(in_avg_bps)
                    full_data_structure[date_key][formatted_time_from_api]["_raw_incoming"] = int(in_avg_bps_float)
                    full_data_structure[date_key][formatted_time_from_api]["In_Averagebps"] = f"{int(in_avg_bps_float):,}"
                except (ValueError, TypeError):
                    full_data_structure[date_key][formatted_time_from_api]["_raw_incoming"] = 0
                    full_data_structure[date_key][formatted_time_from_api]["In_Averagebps"] = str(in_avg_bps)

                try:
                    out_avg_bps_float = float(out_avg_bps)
                    full_data_structure[date_key][formatted_time_from_api]["_raw_outcoming"] = int(out_avg_bps_float)
                    full_data_structure[date_key][formatted_time_from_api]["Out_Averagebps"] = f"{int(out_avg_bps_float):,}"
                except (ValueError, TypeError):
                    full_data_structure[date_key][formatted_time_from_api]["_raw_outcoming"] = 0
                    full_data_structure[date_key][formatted_time_from_api]["Out_Averagebps"] = str(out_avg_bps)

                item_bandwidth_raw = item.get(column_mapping["‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)"])
                if item_bandwidth_raw:
                    if "FTTx" in str(item_bandwidth_raw):
                        full_data_structure[date_key][formatted_time_from_api]["‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)"] = "20 Mbps."
                    else:
                        try:
                            numeric_value_match = re.search(r'[\d.]+', str(item_bandwidth_raw))
                            if numeric_value_match:
                                numeric_value = float(numeric_value_match.group())
                                full_data_structure[date_key][formatted_time_from_api]["‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)"] = f"{int(numeric_value)} Mbps."
                            else:
                                full_data_structure[date_key][formatted_time_from_api]["‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)"] = str(item_bandwidth_raw)
                        except (ValueError, TypeError, AttributeError):
                            full_data_structure[date_key][formatted_time_from_api]["‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)"] = str(item_bandwidth_raw)

    processed_data = []
    total_incoming_sum = 0
    total_outcoming_sum = 0
    data_points_count = 0

    for date_key in sorted(full_data_structure.keys()):
        for time_key in sorted(full_data_structure[date_key].keys()):
            row = full_data_structure[date_key][time_key]
            processed_data.append(row)
            total_incoming_sum += row.get("_raw_incoming", 0)
            total_outcoming_sum += row.get("_raw_outcoming", 0)
            data_points_count += 1

    average_incoming = 0
    average_outcoming = 0
    if data_points_count > 0:
        average_incoming = round(total_incoming_sum / data_points_count)
        average_outcoming = round(total_outcoming_sum / data_points_count)

    grand_total_row = {
        "‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô": "Grand Total",
        "‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô": "",
        "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤": "",
        "‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)": "",
        "In_Averagebps": f"{average_incoming:,}",
        "Out_Averagebps": f"{average_outcoming:,}"
    }

    return desired_headers_th, processed_data, grand_total_row



def export_to_csv(headers, data, filename, job_id, node_name):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV
    Parameters:
    - headers (list): ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
    - data (list): ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á CSV (‡∏£‡∏ß‡∏° Grand Total ‡πÅ‡∏•‡πâ‡∏ß)
    - filename (str): ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    - job_id (str): ID ‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö logging)
    - node_name (str): ‡∏ä‡∏∑‡πà‡∏≠ Node (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö logging)
    Returns:
    - tuple: (bool, str) -> (True ‡∏´‡∏≤‡∏Å‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à, ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞)
    """
    try:
        # ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏´‡∏°‡∏î 'w' (write), 'newline=''' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ß‡πà‡∏≤‡∏á, 'utf-8-sig' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö BOM (Byte Order Mark)
        # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Excel ‡πÄ‡∏õ‡∏¥‡∏î‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
            cw = csv.writer(f) # ‡∏™‡∏£‡πâ‡∏≤‡∏á CSV writer object
            if headers and data:
                cw.writerow(headers) # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
                # ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡πá‡∏ö last_customer_id/name ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CSV ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏∏‡∏Å‡πÅ‡∏ñ‡∏ß
                for row in data:
                    new_row = [
                        row.get('‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô', ''),
                        row.get('‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô', ''),
                        row.get('‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤', ''),
                        row.get('‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)', ''),
                        row.get('In_Averagebps', ''),
                        row.get('Out_Averagebps', '')
                    ]
                    cw.writerow(new_row) # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß
            else:
                cw.writerow(["No Data"]) # ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        logger.info(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á CSV ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{node_name}' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß")
        return True, "Success"
    except Exception as e:
        logger.error(f"‚ùå ‡∏™‡∏£‡πâ‡∏≤‡∏á CSV ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{node_name}' ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
        return False, str(e)

def export_to_pdf(headers, daily_data, grand_total_row, filename, job_id, node_name):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå PDF ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏±‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà, Grand Total ‡∏≠‡∏¢‡∏π‡πà‡∏ï‡πà‡∏≠‡∏ó‡πâ‡∏≤‡∏¢‡∏ß‡∏±‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• "‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô" ‡πÅ‡∏•‡∏∞ "‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô" ‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ï‡πà‡∏≠‡∏ß‡∏±‡∏ô (‡∏ñ‡πâ‡∏≤‡∏ã‡πâ‡∏≥)
    Parameters:
    - headers (list): ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
    - daily_data (list): ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á PDF (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° Grand Total)
    - grand_total_row (dict): ‡πÅ‡∏ñ‡∏ß Grand Total (Average)
    - filename (str): ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå PDF ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    - job_id (str): ID ‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö logging)
    - node_name (str): ‡∏ä‡∏∑‡πà‡∏≠ Node (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö logging)
    Returns:
    - tuple: (True ‡∏´‡∏≤‡∏Å‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à, ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞)
    """
    try:
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏ö‡∏Å‡∏£‡∏∞‡∏î‡∏≤‡∏©‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡πÉ‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢ inch)
        margin_size = 0.5 * inch  # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏Ç‡∏≠‡∏ö 0.5 ‡∏ô‡∏¥‡πâ‡∏ß ‡∏ó‡∏∏‡∏Å‡∏î‡πâ‡∏≤‡∏ô

        doc = SimpleDocTemplate(
            filename,
            pagesize=letter,
            leftMargin=margin_size,
            rightMargin=margin_size,
            topMargin=margin_size,
            bottomMargin=margin_size
        ) # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ PDF, ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô Letter ‡πÅ‡∏•‡∏∞‡∏Ç‡∏≠‡∏ö‡∏Å‡∏£‡∏∞‡∏î‡∏≤‡∏©

        styles = getSampleStyleSheet()
        elements = []

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á ParagraphStyle ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÄ‡∏ã‡∏•‡∏•‡πå
        cell_paragraph_style = ParagraphStyle('TableCellParagraph', parent=styles['Normal'])
        if THAI_FONT_REGISTERED:
            cell_paragraph_style.fontName = THAI_FONT_NAME
        cell_paragraph_style.fontSize = 10
        cell_paragraph_style.alignment = 1 # ‡∏à‡∏±‡∏î‡∏Å‡∏∂‡πà‡∏á‡∏Å‡∏•‡∏≤‡∏á (‡∏´‡∏£‡∏∑‡∏≠ LEFT/RIGHT ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)


        if headers and daily_data:
            data_by_date = {}
            for row in daily_data:
                date_time_str = row.get('‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤', '')
                try:
                    date_key = datetime.datetime.strptime(date_time_str, '%Y-%m-%d %H.%M.%S').strftime('%Y-%m-%d')
                except ValueError:
                    date_key = 'Uncategorized'
                    logger.warning(f"Found uncategorized date for PDF: {date_time_str}")
                if date_key not in data_by_date:
                    data_by_date[date_key] = []
                data_by_date[date_key].append(row)

            report_month_str = "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏î‡∏∑‡∏≠‡∏ô"
            if data_by_date:
                first_date_str = sorted(data_by_date.keys())[0]
                try:
                    first_date_obj = datetime.datetime.strptime(first_date_str, '%Y-%m-%d')
                    thai_months = {
                        1: "‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°", 2: "‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå", 3: "‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°", 4: "‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô",
                        5: "‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°", 6: "‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô", 7: "‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°", 8: "‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°",
                        9: "‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô", 10: "‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°", 11: "‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô", 12: "‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°"
                    }
                    report_month_str = thai_months.get(first_date_obj.month, "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏î‡∏∑‡∏≠‡∏ô")
                except ValueError:
                    logger.warning(f"Could not parse first date for month determination: {first_date_str}")

            first_page = True
            sorted_dates = sorted(data_by_date.keys())

            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á
            # letter width = 8.5 inches
            # Usable width = page_width - leftMargin - rightMargin
            # letter page: 8.5 * inch
            usable_page_width = letter[0] - (2 * margin_size) # letter[0] ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏£‡∏∞‡∏î‡∏≤‡∏©

            for i, date_key in enumerate(sorted_dates):
                group_data = data_by_date[date_key]

                if not first_page:
                    elements.append(PageBreak())

                title_style = styles['Title']
                if THAI_FONT_REGISTERED:
                    title_style.fontName = THAI_FONT_NAME
                title_style.fontSize = 20
                title_style.alignment = 1
                elements.append(Paragraph("Customer Interface Summary Report by Hour", title_style))
                elements.append(Spacer(1, 0.2 * inch))

                month_report_style = ParagraphStyle('MonthReport', parent=styles['Normal'])
                if THAI_FONT_REGISTERED:
                    month_report_style.fontName = THAI_FONT_NAME
                month_report_style.fontSize = 18
                month_report_style.alignment = 1
                elements.append(Paragraph(f"‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡πÄ‡∏î‡∏∑‡∏≠‡∏ô {report_month_str}", month_report_style))
                elements.append(Spacer(1, 0.2 * inch))

                date_header_style = ParagraphStyle('DateHeader', parent=styles['Normal'])
                if THAI_FONT_REGISTERED:
                    date_header_style.fontName = THAI_FONT_NAME
                date_header_style.fontSize = 16
                date_header_style.alignment = 1
                elements.append(Paragraph(f"<b>‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà </b> {date_key}", date_header_style))
                elements.append(Spacer(1, 0.2 * inch))

                table_headers = [
                    "‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô",
                    "‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô",
                    "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤",
                    "‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)",
                    "‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô incoming (‡∏´‡∏ô‡πà‡∏ß‡∏¢ bps)",
                    "‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô outcoming (‡∏´‡∏ô‡πà‡∏ß‡∏¢ bps)"
                ]
                
                # *** ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå (colWidths) ***
                # ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ (usable_page_width)
                # ‡∏õ‡∏£‡∏±‡∏ö‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
                col_widths = [
                    0.10 * usable_page_width, # ‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô (10% ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ)
                    0.25 * usable_page_width, # ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô (25% ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ)
                    0.15 * usable_page_width, # ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤
                    0.15 * usable_page_width, # ‡∏Ç‡∏ô‡∏≤‡∏îBandwidth
                    0.175 * usable_page_width, # incoming
                    0.175 * usable_page_width  # outcoming
                ]
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö 1.0 (‡∏´‡∏£‡∏∑‡∏≠ 100%)
                # ‡πÄ‡∏ä‡πà‡∏ô 0.10 + 0.25 + 0.15 + 0.15 + 0.175 + 0.175 = 1.0

                # ‡∏™‡∏£‡πâ‡∏≤‡∏á header row ‡∏î‡πâ‡∏ß‡∏¢ Paragraph Objects ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡πÑ‡∏ó‡∏¢‡πÑ‡∏î‡πâ
                header_row_with_paragraphs = []
                header_paragraph_style = ParagraphStyle('TableHeaderParagraph', parent=styles['Normal'])
                if THAI_FONT_REGISTERED:
                    header_paragraph_style.fontName = THAI_FONT_NAME
                header_paragraph_style.fontSize = 10
                header_paragraph_style.alignment = 1 # ‡∏à‡∏±‡∏î‡∏Å‡∏∂‡πà‡∏á‡∏Å‡∏•‡∏≤‡∏á‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á
                header_paragraph_style.fontName = THAI_FONT_NAME if THAI_FONT_REGISTERED else 'Helvetica-Bold'

                for header_text in table_headers:
                    header_row_with_paragraphs.append(Paragraph(header_text, header_paragraph_style))

                table_data = [header_row_with_paragraphs]
                
                # Track rows for spanning
                span_data = {
                    '‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô': {'start_row': -1, 'value': None},
                    '‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô': {'start_row': -1, 'value': None}
                }
                
                # List to store ReportLab table style commands
                table_styles_commands = []

                for idx_row, row in enumerate(group_data):
                    current_customer_id = row.get('‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô', '')
                    current_customer_name = row.get('‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô', '')
                    current_bandwidth = row.get('‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)', '')

                    cell_data_row = [
                        Paragraph(current_customer_id, cell_paragraph_style),
                        Paragraph(current_customer_name, cell_paragraph_style),
                        Paragraph(row.get('‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤', ''), cell_paragraph_style),
                        Paragraph(str(current_bandwidth), cell_paragraph_style),
                        Paragraph(str(row.get('In_Averagebps', '')), cell_paragraph_style),
                        Paragraph(str(row.get('Out_Averagebps', '')), cell_paragraph_style)
                    ]
                    table_data.append(cell_data_row)

                    # Calculate actual row index in the table (offset by 1 for header row)
                    current_table_row_index = idx_row + 1 

                    # Logic for spanning "‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô" (Column 0)
                    if span_data['‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']['value'] is None:
                        span_data['‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']['value'] = current_customer_id
                        span_data['‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']['start_row'] = current_table_row_index
                    elif current_customer_id != span_data['‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']['value']:
                        if current_table_row_index - 1 > span_data['‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']['start_row']:
                            table_styles_commands.append(('SPAN', (0, span_data['‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']['start_row']), (0, current_table_row_index - 1)))
                        span_data['‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']['value'] = current_customer_id
                        span_data['‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']['start_row'] = current_table_row_index
                    
                    # Logic for spanning "‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô" (Column 1)
                    if span_data['‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']['value'] is None:
                        span_data['‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']['value'] = current_customer_name
                        span_data['‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']['start_row'] = current_table_row_index
                    elif current_customer_name != span_data['‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']['value']:
                        if current_table_row_index - 1 > span_data['‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']['start_row']:
                            table_styles_commands.append(('SPAN', (1, span_data['‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']['start_row']), (1, current_table_row_index - 1)))
                        span_data['‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']['value'] = current_customer_name
                        span_data['‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']['start_row'] = current_table_row_index

                # After iterating through all rows for the current day, close any open spans
                if span_data['‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']['start_row'] != -1 and len(group_data) + 1 > span_data['‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']['start_row']:
                    table_styles_commands.append(('SPAN', (0, span_data['‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']['start_row']), (0, len(group_data))))
                if span_data['‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']['start_row'] != -1 and len(group_data) + 1 > span_data['‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']['start_row']:
                    table_styles_commands.append(('SPAN', (1, span_data['‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']['start_row']), (1, len(group_data))))

                # If it's the last day and there's a Grand Total row, add it to the table data
                if i == len(sorted_dates) - 1 and grand_total_row:
                    grand_total_row_data = [
                        Paragraph(grand_total_row.get('‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô', ''), cell_paragraph_style),
                        Paragraph(grand_total_row.get('‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô', ''), cell_paragraph_style),
                        Paragraph(grand_total_row.get('‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤', ''), cell_paragraph_style),
                        Paragraph(str(grand_total_row.get('‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)', '')), cell_paragraph_style),
                        Paragraph(str(grand_total_row.get('In_Averagebps', '')), cell_paragraph_style),
                        Paragraph(str(grand_total_row.get('Out_Averagebps', '')), cell_paragraph_style)
                    ]
                    table_data.append(grand_total_row_data)

                table = Table(table_data, colWidths=col_widths)
                table_style = [
                    ('BACKGROUND', (0, 0), (-1, 0), '#cccccc'),
                    ('TEXTCOLOR', (0, 0), (-1, 0), '#000000'),
                    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                    ('BACKGROUND', (0, 1), (-1, -1), "#ffffff"),
                    ('GRID', (0, 0), (-1, -1), 1, '#999999'),
                    ('FONTSIZE', (0, 0), (-1, -1), 10), # Font size is now largely controlled by ParagraphStyle
                    ('LEFTPADDING', (0,0), (-1,-1), 6),
                    ('RIGHTPADDING', (0,0), (-1,-1), 6),
                    ('VALIGN', (0,0), (-1,-1), 'TOP'),
                ]

                if THAI_FONT_REGISTERED:
                    table_style.append(('FONTNAME', (0, 0), (-1, 0), THAI_FONT_NAME))
                else:
                    table_style.append(('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'))

                table_style.extend(table_styles_commands)

                if i == len(sorted_dates) - 1 and grand_total_row:
                    grand_total_row_index = len(table_data) - 1
                    table_style.append(('BACKGROUND', (0, grand_total_row_index), (-1, grand_total_row_index), '#dddddd'))
                    table_style.append(('FONTNAME', (0, grand_total_row_index), (-1, grand_total_row_index), THAI_FONT_NAME if THAI_FONT_REGISTERED else 'Helvetica-Bold'))
                    table_style.append(('SPAN', (0, grand_total_row_index), (3, grand_total_row_index)))
                    table_style.append(('ALIGN', (0, grand_total_row_index), (3, grand_total_row_index), 'LEFT'))
                    table_style.append(('VALIGN', (0, grand_total_row_index), (-1, grand_total_row_index), 'MIDDLE'))

                table.setStyle(TableStyle(table_style))
                elements.append(table)
                elements.append(Spacer(1, 0.5 * inch))

                first_page = False

        else:
            no_data_style = styles['Normal']
            if THAI_FONT_REGISTERED:
                no_data_style.fontName = THAI_FONT_NAME
            elements.append(Paragraph("No circuit status data available.", no_data_style))

        doc.build(elements)
        logger.info(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á PDF ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{node_name}' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß")
        return True, "PDF generated successfully."
    except Exception as e:
        logger.error(f"‚ùå ‡∏™‡∏£‡πâ‡∏≤‡∏á PDF ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{node_name}' ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
        return False, f"Error generating PDF: {e}"


def process_file_in_background(file_stream, job_id):
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏≠‡∏µ‡∏Å Thread ‡∏´‡∏ô‡∏∂‡πà‡∏á (background process)
    ‡πÇ‡∏î‡∏¢‡∏à‡∏∞‡∏£‡∏±‡∏ö file_stream (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ü‡∏•‡πå Excel) ‡πÅ‡∏•‡∏∞ job_id ‡∏°‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
    ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel, ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å API, ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•, ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå CSV/PDF
    ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏∞ Zip ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô
    """
    temp_dir = None # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö path ‡∏Ç‡∏≠‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
    try:
        df = pd.read_excel(file_stream) # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel ‡∏î‡πâ‡∏ß‡∏¢ Pandas
        total_rows = len(df) # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô Excel

        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏á‡∏≤‡∏ô (thread-safe)
        with status_lock:
            processing_status[job_id]['total'] = total_rows
            processing_status[job_id]['results'] = [] # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ Node/Interface
            temp_dir = tempfile.mkdtemp(prefix=f"report_job_{job_id}_") # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
            processing_status[job_id]['temp_dir'] = temp_dir # ‡πÄ‡∏Å‡πá‡∏ö path ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏á‡∏≤‡∏ô

        logger.info(f"üìä ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå Excel ‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {total_rows} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏µ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡πÉ‡∏ô Excel ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        required_columns = ['NodeID', 'Interface ID', '‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á / ‡∏™‡∏±‡∏á‡∏Å‡∏±‡∏î', '‡∏Å‡∏£‡∏° / ‡∏™‡∏±‡∏á‡∏Å‡∏±‡∏î', '‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î', '‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô', 'Node Name']
        if not all(col in df.columns for col in required_columns):
            missing_cols = [c for c in required_columns if c not in df.columns]
            with status_lock:
                processing_status[job_id]['error'] = f"‡πÑ‡∏ü‡∏•‡πå Excel ‡∏Ç‡∏≤‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô: {', '.join(missing_cols)}"
                processing_status[job_id]['completed'] = True # ‡∏ï‡∏±‡πâ‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏ï‡πà‡∏°‡∏µ error
            logger.error(f"‚ùå {processing_status[job_id]['error']}")
            return # ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Thread ‡∏ô‡∏µ‡πâ

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö CSV ‡πÅ‡∏•‡∏∞ PDF ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
        csv_root_dir = os.path.join(temp_dir, 'CSV')
        pdf_root_dir = os.path.join(temp_dir, 'PDF')
        os.makedirs(csv_root_dir, exist_ok=True) # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
        os.makedirs(pdf_root_dir, exist_ok=True)

        # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß‡πÉ‡∏ô DataFrame (‡πÅ‡∏ï‡πà‡∏•‡∏∞ Node/Interface)
        for index, row in df.iterrows():
            with status_lock:
                if processing_status[job_id].get('canceled'): # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏á‡∏≤‡∏ô‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                    logger.info(f"‚õî ‡∏á‡∏≤‡∏ô‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡πÇ‡∏î‡∏¢‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ")
                    break # ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏•‡∏π‡∏õ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å

            node_name = '' # ‡∏ä‡∏∑‡πà‡∏≠ Node ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ logging ‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
            csv_success = False # ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á CSV
            pdf_success = False # ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á PDF
            error_message = None # ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° error ‡∏´‡∏≤‡∏Å‡∏°‡∏µ

            try:
                nod_id = str(row['NodeID']).strip() # Node ID
                itf_id = str(row['Interface ID']).strip() # Interface ID

                # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå
                folder1 = str(row['‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á / ‡∏™‡∏±‡∏á‡∏Å‡∏±‡∏î']).strip()
                folder2 = str(row['‡∏Å‡∏£‡∏° / ‡∏™‡∏±‡∏á‡∏Å‡∏±‡∏î']).strip()
                folder3 = str(row['‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î']).strip()
                folder4 = str(row['‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']).strip()
                node_name = str(row['Node Name']).strip()

                if not nod_id or not itf_id:
                    error_message = "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NodeID ‡∏´‡∏£‡∏∑‡∏≠ Interface ID ‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå"
                    logger.warning(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà {index + 1} ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å {error_message} (NodeID: '{nod_id}', ITF ID: '{itf_id}')")
                    with status_lock:
                        processing_status[job_id]['processed'] += 1 # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß
                        processing_status[job_id]['results'].append({ # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡πÅ‡∏ñ‡∏ß‡∏ô‡∏µ‡πâ
                            'node_name': node_name,
                            'csv_success': False,
                            'pdf_success': False,
                            'error_message': error_message
                        })
                    continue # ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÅ‡∏ñ‡∏ß‡∏ñ‡∏±‡∏î‡πÑ‡∏õ

                logger.info(f"‚ñ∂ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• NodeID: {nod_id}, Interface ID: {itf_id} (‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà {index + 1})")

                # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Path ‡∏Ç‡∏≠‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CSV ‡πÅ‡∏•‡∏∞ PDF ‡∏Ç‡∏≠‡∏á Node/Interface ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
                current_csv_dir = os.path.join(csv_root_dir, folder1, folder2, folder3, folder4)
                current_pdf_dir = os.path.join(pdf_root_dir, folder1, folder2, folder3, folder4)

                os.makedirs(current_csv_dir, exist_ok=True)
                os.makedirs(current_pdf_dir, exist_ok=True)

                raw_json_data = get_data_from_api(nod_id, itf_id, job_id) # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å API

                if raw_json_data:
                    # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CSV/PDF
                    headers, processed_daily_data, grand_total_row_data = process_json_data(raw_json_data, job_id, nod_id, folder4)

                    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î Node Name ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå (‡∏•‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå)
                    sanitized_node_name = re.sub(r'[\\/:*?"<>|]', '_', node_name)
                    filename_base = f"{sanitized_node_name}"

                    csv_filename = os.path.join(current_csv_dir, f"{filename_base}.csv")
                    pdf_filename = os.path.join(current_pdf_dir, f"{filename_base}.pdf")

                    # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CSV: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß + ‡πÅ‡∏ñ‡∏ß Grand Total
                    csv_data_to_write = list(processed_daily_data) # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏ô‡∏≤
                    if grand_total_row_data:
                        csv_data_to_write.append(grand_total_row_data)

                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå CSV ‡πÅ‡∏•‡∏∞ PDF
                    csv_success, csv_msg = export_to_csv(headers, csv_data_to_write, csv_filename, job_id, node_name)
                    pdf_success, pdf_msg = export_to_pdf(headers, processed_daily_data, grand_total_row_data, pdf_filename, job_id, node_name)
                else:
                    error_message = f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å API ‡πÑ‡∏î‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NodeID: {nod_id}, Interface ID: {itf_id}"
                    logger.error(f"‚ùå {error_message}")

            except Exception as e:
                # ‡∏î‡∏±‡∏Å‡∏à‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß
                error_message = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î‡πÉ‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà {index + 1}: {e}"
                logger.error(f"‚ùå {error_message}")

            finally:
                # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß
                with status_lock:
                    processing_status[job_id]['processed'] += 1
                    processing_status[job_id]['results'].append({
                        'node_name': node_name,
                        'csv_success': csv_success,
                        'pdf_success': pdf_success,
                        'error_message': error_message
                    })

        # ‡∏´‡∏≤‡∏Å‡∏á‡∏≤‡∏ô‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ó‡∏∏‡∏Å‡πÅ‡∏ñ‡∏ß‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå ZIP
        if not processing_status[job_id].get('canceled'):
            # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î
            today_date = datetime.datetime.now().strftime('%Y%m%d')
            download_name = f"{today_date}_SummaryReportbyHour.zip" # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
            zip_filename_path = os.path.join(temp_dir, download_name)

            if temp_dir and os.path.exists(temp_dir):
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå ZIP ‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (CSV ‡πÅ‡∏•‡∏∞ PDF)
                with zipfile.ZipFile(zip_filename_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for root, _, files in os.walk(temp_dir):
                        for file in files:
                            file_path = os.path.join(root, file)
                            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ö‡∏µ‡∏ö‡∏≠‡∏±‡∏î‡πÑ‡∏ü‡∏•‡πå ZIP ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏≠‡∏¢‡∏π‡πà
                            if file_path != zip_filename_path: 
                                arcname = os.path.relpath(file_path, temp_dir)
                                zipf.write(file_path, arcname)

                with status_lock:
                    status = processing_status.get(job_id)
                    if status:
                        status['zip_file_path'] = zip_filename_path
                        status['download_name'] = download_name  # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î
                        status['completed'] = True
                    else:
                        logger.error(f"Job {job_id} not found in status list.")
                        return False, "Job not found"

                return True, "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ö‡∏µ‡∏ö‡∏≠‡∏±‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß"

    except Exception as e:
        # ‡∏î‡∏±‡∏Å‡∏à‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô process_file_in_background ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        with status_lock:
            processing_status[job_id]['error'] = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á: {e}"
            processing_status[job_id]['completed'] = True
        logger.critical(f"‚ùå {processing_status[job_id]['error']}")

    finally:
        # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡πâ‡∏•‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏¢‡πà‡∏≠‡∏¢
        # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå ZIP ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏´‡∏•‡∏±‡∏Å‡πÑ‡∏ß‡πâ
        if csv_root_dir and os.path.exists(csv_root_dir):
            shutil.rmtree(csv_root_dir, ignore_errors=True)
        if pdf_root_dir and os.path.exists(pdf_root_dir):
            shutil.rmtree(pdf_root_dir, ignore_errors=True)

# --- Flask Routes ---
@app.route('/')
def upload_form():
    """‡πÅ‡∏™‡∏î‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Excel (index.html)"""
    return render_template('index.html')

@app.route('/generate_report', methods=['POST'])
def generate_report():
    """
    ‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå Excel ‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÉ‡∏ô‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á (new thread)
    """
    if 'excel_file' not in request.files:
        return jsonify({"error": "No file part"}), 400 # HTTP 400 Bad Request
    
    file = request.files['excel_file']
    if file.filename == '':
        return jsonify({"error": "No selected file"}), 400
    
    if file:
        job_id = str(uuid.uuid4()) # ‡∏™‡∏£‡πâ‡∏≤‡∏á Unique ID ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ô‡∏µ‡πâ
        file_stream = io.BytesIO(file.read()) # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡πá‡∏ô BytesIO ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Thread ‡∏≠‡∏∑‡πà‡∏ô

        # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà (thread-safe)
        with status_lock:
            processing_status[job_id] = {
                'total': -1, # ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
                'processed': 0, # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß
                'completed': False, # ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
                'error': None, # ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° error ‡∏´‡∏≤‡∏Å‡∏°‡∏µ
                'canceled': False, # ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å
                'results': [], # ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
                'temp_dir': None, # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
                'zip_file_path': None, # Path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå ZIP
                'timestamp': datetime.datetime.now() # ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏á‡∏≤‡∏ô
            }
        logger.info(f"üìÇ ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå excel '{file.filename}' ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•")

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏° Thread ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á
        thread = threading.Thread(target=process_file_in_background, args=(file_stream, job_id))
        thread.daemon = True # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ Thread ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏•‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏à‡∏ö
        thread.start()

        # ‡∏™‡πà‡∏á Job ID ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÉ‡∏´‡πâ Client ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
        return jsonify({"message": "Processing started", "job_id": job_id})

@app.route('/status/<job_id>')
def get_status(job_id):
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏≠‡∏¢‡∏π‡πà
    Client ‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï UI
    """
    with status_lock:
        status = processing_status.get(job_id, {}) # ‡∏î‡∏∂‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏á‡∏≤‡∏ô (thread-safe)
    return jsonify(status)

@app.route('/logs/<job_id>')
def get_logs(job_id):
    """
    ‡∏î‡∏∂‡∏á log ‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏≤‡∏Å Queue
    Client ‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API ‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á log ‡πÅ‡∏ö‡∏ö Real-time
    """
    logs = []
    # ‡∏î‡∏∂‡∏á log ‡∏à‡∏≤‡∏Å queue ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏ß‡πà‡∏≤‡∏á
    while not log_queue.empty():
        try:
            logs.append(log_queue.get_nowait()) # get_nowait() ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏ñ‡πâ‡∏≤ queue ‡∏ß‡πà‡∏≤‡∏á
        except Exception:
            break
    return jsonify({"logs": logs})


@app.route('/cancel/<job_id>', methods=['POST'])
def cancel_job(job_id):
    """
    ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏≠‡∏¢‡∏π‡πà
    """
    with status_lock:
        if job_id in processing_status:
            processing_status[job_id]['canceled'] = True # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ flag 'canceled' ‡πÄ‡∏õ‡πá‡∏ô True
            logger.info(f"‚õî ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ç‡∏≠‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏á‡∏≤‡∏ô")
            return jsonify({"message": "Job cancellation requested"}), 200
        else:
            logger.warning(f"‚ö†Ô∏è ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏û‡∏ö")
            return jsonify({"error": "Job not found"}), 404

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/download_converted_excel/<job_id>')
def download_converted_excel(job_id):
    """
    ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Excel (CSV) ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡πâ‡∏ß
    """
    with status_lock:
        status = processing_status.get(job_id)
        if not status:
            return jsonify({'error': 'Job ID not found'}), 404

        csv_file_path = status['csv_file_path']
        if not csv_file_path or not os.path.exists(csv_file_path):
            return jsonify({'error': 'File not found'}), 404

    try:
        # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å path ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î
        filename = os.path.basename(csv_file_path)
        return send_file(csv_file_path, as_attachment=True, download_name=filename)
    except Exception as e:
        logger.error(f"Error downloading file for job {job_id}: {e}")
        return jsonify({'error': 'An error occurred during file download'}), 500

@app.route('/download_report/<job_id>')
def download_report(job_id):
    """
    ‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå ZIP ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
    """
    with status_lock:
        status_entry = processing_status.get(job_id)

    if status_entry and status_entry['completed'] and status_entry['zip_file_path']:
        # ‡πÅ‡∏¢‡∏Å‡∏û‡∏≤‡∏ò‡∏Ç‡∏≠‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Å‡∏±‡∏ô
        temp_dir = os.path.dirname(status_entry['zip_file_path'])
        zip_filename = os.path.basename(status_entry['zip_file_path'])
        download_name_final = status_entry['download_name']

        # ‡πÉ‡∏ä‡πâ send_from_directory ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        return send_from_directory(
            directory=temp_dir,         # ‡∏™‡πà‡∏á‡∏û‡∏≤‡∏ò‡∏Ç‡∏≠‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
            path=zip_filename,          # ‡∏™‡πà‡∏á‡πÅ‡∏Ñ‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
            as_attachment=True,
            download_name=download_name_final # ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î
        )
    else:
        logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå ZIP ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏£‡πá‡∏à. Path: {status_entry.get('zip_file_path')}")
        return jsonify({"error": "File not found or report not completed."}), 404

def cleanup_old_jobs():
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏•‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå ZIP ‡πÄ‡∏Å‡πà‡∏≤‡πÜ ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö
    ‡∏£‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô background process ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ threading.Timer
    """
    #logger.info("üßπ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏á‡∏≤‡∏ô‡πÄ‡∏Å‡πà‡∏≤...")
    current_time = datetime.datetime.now()
    jobs_to_remove = [] # list ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö job_id ‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏•‡∏ö

    retention_hours = 24 # ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏Å‡πá‡∏ö‡∏á‡∏≤‡∏ô (‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ 24 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á)
    retention_seconds = retention_hours * 3600

    with status_lock:
        # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ú‡πà‡∏≤‡∏ô‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô processing_status
        for job_id, job_info in list(processing_status.items()): # ‡πÉ‡∏ä‡πâ list(items()) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏ö item ‡∏Ç‡∏ì‡∏∞‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡πÑ‡∏î‡πâ
            if job_info.get('completed') and job_info.get('timestamp'):
                job_timestamp = job_info['timestamp']
                # ‡∏ñ‡πâ‡∏≤‡∏á‡∏≤‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡∏¥‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î ‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏ô jobs_to_remove
                if (current_time - job_timestamp).total_seconds() > retention_seconds:
                    jobs_to_remove.append(job_id)
            # ‡∏ñ‡πâ‡∏≤‡∏á‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå ‡πÅ‡∏•‡∏∞‡∏Ñ‡πâ‡∏≤‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô 1/4 ‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤ retention ‡πÉ‡∏´‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏Ñ‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡∏≠‡∏≠‡∏Å
            elif (not job_info.get('completed')) and (current_time - job_info.get('timestamp', current_time)).total_seconds() > (retention_seconds / 4):
                logger.warning(f"‚ö†Ô∏è ‡∏û‡∏ö‡∏á‡∏≤‡∏ô‡∏Ñ‡πâ‡∏≤‡∏á‡πÄ‡∏Å‡πà‡∏≤ (‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå)")
                jobs_to_remove.append(job_id)


    for job_id in jobs_to_remove:
        with status_lock:
            job_info = processing_status.pop(job_id, None) # ‡∏•‡∏ö‡∏á‡∏≤‡∏ô‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å processing_status
        if job_info:
            zip_file_path = job_info.get('zip_file_path')
            # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå ZIP ‡πÅ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á ‡πÉ‡∏´‡πâ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏±‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢
            if zip_file_path and os.path.exists(zip_file_path):
                try:
                    os.remove(zip_file_path)
                    #logger.info(f"üóëÔ∏è ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå ZIP ‡πÄ‡∏Å‡πà‡∏≤: {os.path.basename(zip_file_path)} (Job ID: {job_id})")
                except Exception as e:
                    logger.error(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå ZIP ‡πÄ‡∏Å‡πà‡∏≤: {e} ")
            logger.info(f"‚ú® ‡∏•‡πâ‡∏≤‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏á‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Job ID: {job_id} ‡πÅ‡∏•‡πâ‡∏ß")
    #logger.info("üßπ ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏á‡∏≤‡∏ô‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå")
    # ‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏´‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï (‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤ retention)
    threading.Timer(retention_seconds / 2, cleanup_old_jobs).start()

# --- Main Execution Block ---
if __name__ == '__main__':
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Thread ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö cleanup_old_jobs ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô daemon ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Thread ‡∏à‡∏ö‡πÄ‡∏°‡∏∑‡πà‡∏≠ Main Thread ‡∏à‡∏ö
    cleanup_thread = threading.Thread(target=cleanup_old_jobs)
    cleanup_thread.daemon = True
    cleanup_thread.start()

    # ‡∏£‡∏±‡∏ô Flask application
    # debug=True ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ Server ‡∏£‡∏µ‡πÇ‡∏´‡∏•‡∏î‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô ‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á traceback ‡∏ó‡∏µ‡πà‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏∂‡πâ‡∏ô
    app.run(debug=True,host= '0.0.0.0',port=5050)

